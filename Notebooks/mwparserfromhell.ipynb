{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_markup(text):\n",
    "    text = re_section_headers.sub(' ', text)\n",
    "    text = re_whitespace.sub(' ', text)\n",
    "    text = re_display_styles.sub(' ', text)\n",
    "    text = re_duplicate_spaces.sub(' ', text)\n",
    "    \n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable\\'s mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterised. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location.\\nSets of central moments can be defined for both univariate and multivariate distributions.\\n\\n\\n== Univariate moments ==\\nThe nth moment about the mean (or nth central moment) of a real-valued random variable X is the quantity μn := E[(X − E[X])n], where E is the expectation operator. For a continuous univariate probability distribution with probability density function f(x), the nth moment about the mean μ is\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        =\\n        E\\n        \\u2061\\n        \\n          [\\n          \\n            (\\n            X\\n            −\\n            E\\n            \\u2061\\n            [\\n            X\\n            ]\\n            \\n              )\\n              \\n                n\\n              \\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∫\\n          \\n            −\\n            ∞\\n          \\n          \\n            +\\n            ∞\\n          \\n        \\n        (\\n        x\\n        −\\n        μ\\n        \\n          )\\n          \\n            n\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        \\n          d\\n        \\n        x\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\mu _{n}=\\\\operatorname {E} \\\\left[(X-\\\\operatorname {E} [X])^{n}\\\\right]=\\\\int _{-\\\\infty }^{+\\\\infty }(x-\\\\mu )^{n}f(x)\\\\,\\\\mathrm {d} x.}\\n  \\nFor random variables that have no mean, such as the Cauchy distribution, central moments are not defined.\\nThe first few central moments have intuitive interpretations:\\nThe \"zeroth\" central moment μ0 is 1.\\nThe first central moment μ1 is 0 (not to be confused with the first (raw) moment itself, the expected value or mean).\\nThe second central moment μ2 is called the variance, and is usually denoted σ2, where σ represents the standard deviation.\\nThe third and fourth central moments are used to define the standardized moments which are used to define skewness and kurtosis, respectively.\\n\\n\\n=== Properties ===\\nThe nth central moment is translation-invariant, i.e. for any random variable X and any constant c, we have\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        X\\n        +\\n        c\\n        )\\n        =\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        X\\n        )\\n        .\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{n}(X+c)=\\\\mu _{n}(X).\\\\,}\\n  \\nFor all n, the nth central moment is homogeneous of degree n:\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        c\\n        X\\n        )\\n        =\\n        \\n          c\\n          \\n            n\\n          \\n        \\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        X\\n        )\\n        .\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{n}(cX)=c^{n}\\\\mu _{n}(X).\\\\,}\\n  \\nOnly for n such that n equals 1, 2, or 3 do we have an additivity property for random variables X and Y that are independent:\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        X\\n        +\\n        Y\\n        )\\n        =\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        X\\n        )\\n        +\\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        (\\n        Y\\n        )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{n}(X+Y)=\\\\mu _{n}(X)+\\\\mu _{n}(Y)\\\\,}\\n   provided n ∈ {1, 2, 3}.\\nA related functional that shares the translation-invariance and homogeneity properties with the nth central moment, but continues to have this additivity property even when n ≥ 4 is the nth cumulant κn(X). For n = 1, the nth cumulant is just the expected value; for n = either 2 or 3, the nth cumulant is just the nth central moment; for n ≥ 4, the nth cumulant is an nth-degree monic polynomial in the first n moments (about zero), and is also a (simpler) nth-degree polynomial in the first n central moments.\\n\\n\\n=== Relation to moments about the origin ===\\nSometimes it is convenient to convert moments about the origin to moments about the mean. The general equation for converting the nth-order moment about the origin to the moment about the mean is\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            n\\n          \\n        \\n        =\\n        \\n          E\\n        \\n        \\n          [\\n          \\n            \\n              (\\n              \\n                X\\n                −\\n                \\n                  E\\n                \\n                \\n                  [\\n                  X\\n                  ]\\n                \\n              \\n              )\\n            \\n            \\n              n\\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∑\\n          \\n            j\\n            =\\n            0\\n          \\n          \\n            n\\n          \\n        \\n        \\n          \\n            \\n              (\\n            \\n            \\n              n\\n              j\\n            \\n            \\n              )\\n            \\n          \\n        \\n        (\\n        −\\n        1\\n        \\n          )\\n          \\n            n\\n            −\\n            j\\n          \\n        \\n        \\n          μ\\n          \\n            j\\n          \\n          ′\\n        \\n        \\n          μ\\n          \\n            n\\n            −\\n            j\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\mu _{n}=\\\\mathrm {E} \\\\left[\\\\left(X-\\\\mathrm {E} \\\\left[X\\\\right]\\\\right)^{n}\\\\right]=\\\\sum _{j=0}^{n}{n \\\\choose j}(-1)^{n-j}\\\\mu \\'_{j}\\\\mu ^{n-j},}\\n  \\nwhere μ is the mean of the distribution, and the moment about the origin is given by\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            m\\n          \\n          ′\\n        \\n        =\\n        \\n          ∫\\n          \\n            −\\n            ∞\\n          \\n          \\n            +\\n            ∞\\n          \\n        \\n        \\n          x\\n          \\n            m\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        =\\n        \\n          E\\n        \\n        \\n          [\\n          \\n            X\\n            \\n              m\\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∑\\n          \\n            j\\n            =\\n            0\\n          \\n          \\n            m\\n          \\n        \\n        \\n          \\n            \\n              (\\n            \\n            \\n              m\\n              j\\n            \\n            \\n              )\\n            \\n          \\n        \\n        \\n          μ\\n          \\n            j\\n          \\n        \\n        \\n          μ\\n          \\n            m\\n            −\\n            j\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\mu \\'_{m}=\\\\int _{-\\\\infty }^{+\\\\infty }x^{m}f(x)\\\\,dx=\\\\mathrm {E} \\\\left[X^{m}\\\\right]=\\\\sum _{j=0}^{m}{m \\\\choose j}\\\\mu _{j}\\\\mu ^{m-j}.}\\n  \\nFor the cases n = 2, 3, 4 — which are of most interest because of the relations to variance, skewness, and kurtosis, respectively — this formula becomes (noting that \\n  \\n    \\n      \\n        μ\\n        =\\n        \\n          μ\\n          \\n            1\\n          \\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu =\\\\mu \\'_{1}}\\n   and \\n  \\n    \\n      \\n        \\n          μ\\n          \\n            0\\n          \\n          ′\\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\mu \\'_{0}=1}\\n  ):,\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          μ\\n          \\n            2\\n          \\n          ′\\n        \\n        −\\n        \\n          μ\\n          \\n            2\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{2}=\\\\mu \\'_{2}-\\\\mu ^{2}\\\\,}\\n   which is commonly referred to as \\n  \\n    \\n      \\n        \\n          V\\n          a\\n          r\\n        \\n        \\n          (\\n          X\\n          )\\n        \\n        =\\n        \\n          E\\n        \\n        \\n          [\\n          \\n            X\\n            \\n              2\\n            \\n          \\n          ]\\n        \\n        −\\n        \\n          \\n            (\\n            \\n              \\n                E\\n              \\n              \\n                [\\n                X\\n                ]\\n              \\n            \\n            )\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {Var} \\\\left(X\\\\right)=\\\\mathrm {E} \\\\left[X^{2}\\\\right]-\\\\left(\\\\mathrm {E} \\\\left[X\\\\right]\\\\right)^{2}}\\n  \\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            3\\n          \\n        \\n        =\\n        \\n          μ\\n          \\n            3\\n          \\n          ′\\n        \\n        −\\n        3\\n        μ\\n        \\n          μ\\n          \\n            2\\n          \\n          ′\\n        \\n        +\\n        2\\n        \\n          μ\\n          \\n            3\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{3}=\\\\mu \\'_{3}-3\\\\mu \\\\mu \\'_{2}+2\\\\mu ^{3}\\\\,}\\n  \\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            4\\n          \\n        \\n        =\\n        \\n          μ\\n          \\n            4\\n          \\n          ′\\n        \\n        −\\n        4\\n        μ\\n        \\n          μ\\n          \\n            3\\n          \\n          ′\\n        \\n        +\\n        6\\n        \\n          μ\\n          \\n            2\\n          \\n        \\n        \\n          μ\\n          \\n            2\\n          \\n          ′\\n        \\n        −\\n        3\\n        \\n          μ\\n          \\n            4\\n          \\n        \\n        .\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{4}=\\\\mu \\'_{4}-4\\\\mu \\\\mu \\'_{3}+6\\\\mu ^{2}\\\\mu \\'_{2}-3\\\\mu ^{4}.\\\\,}\\n  \\n... and so on, following Pascal\\'s triangle, i.e.\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            5\\n          \\n        \\n        =\\n        \\n          μ\\n          \\n            5\\n          \\n          ′\\n        \\n        −\\n        5\\n        μ\\n        \\n          μ\\n          \\n            4\\n          \\n          ′\\n        \\n        +\\n        10\\n        \\n          μ\\n          \\n            2\\n          \\n        \\n        \\n          μ\\n          \\n            3\\n          \\n          ′\\n        \\n        −\\n        10\\n        \\n          μ\\n          \\n            3\\n          \\n        \\n        \\n          μ\\n          \\n            2\\n          \\n          ′\\n        \\n        +\\n        4\\n        \\n          μ\\n          \\n            5\\n          \\n        \\n        .\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mu _{5}=\\\\mu \\'_{5}-5\\\\mu \\\\mu \\'_{4}+10\\\\mu ^{2}\\\\mu \\'_{3}-10\\\\mu ^{3}\\\\mu \\'_{2}+4\\\\mu ^{5}.\\\\,}\\n  \\nbecause \\n  \\n    \\n      \\n        5\\n        \\n          μ\\n          \\n            4\\n          \\n        \\n        \\n          μ\\n          \\n            1\\n          \\n          ′\\n        \\n        −\\n        \\n          μ\\n          \\n            5\\n          \\n        \\n        \\n          μ\\n          \\n            0\\n          \\n          ′\\n        \\n        =\\n        5\\n        \\n          μ\\n          \\n            4\\n          \\n        \\n        μ\\n        −\\n        \\n          μ\\n          \\n            5\\n          \\n        \\n        =\\n        5\\n        \\n          μ\\n          \\n            5\\n          \\n        \\n        −\\n        \\n          μ\\n          \\n            5\\n          \\n        \\n        =\\n        4\\n        \\n          μ\\n          \\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 5\\\\mu ^{4}\\\\mu \\'_{1}-\\\\mu ^{5}\\\\mu \\'_{0}=5\\\\mu ^{4}\\\\mu -\\\\mu ^{5}=5\\\\mu ^{5}-\\\\mu ^{5}=4\\\\mu ^{5}}\\n  \\nThe following sum is a stochastic variable having a compound distribution\\n\\n  \\n    \\n      \\n        W\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            M\\n          \\n        \\n        \\n          Y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle W=\\\\sum _{i=1}^{M}Y_{i}}\\n  ,\\nwhere the \\n  \\n    \\n      \\n        \\n          Y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Y_{i}}\\n   are mutually independent random variables sharing the same common distribution and \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n   a random integer variable independent of the \\n  \\n    \\n      \\n        \\n          Y\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Y_{k}}\\n   with its own distribution. The moments of \\n  \\n    \\n      \\n        W\\n      \\n    \\n    {\\\\displaystyle W}\\n   are obtained as \\n\\n  \\n    \\n      \\n        \\n          E\\n        \\n        \\n          [\\n          \\n            W\\n            \\n              n\\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n          \\n        \\n        \\n          E\\n        \\n        \\n          [\\n          \\n            \\n              \\n                (\\n              \\n              \\n                M\\n                i\\n              \\n              \\n                )\\n              \\n            \\n          \\n          ]\\n        \\n        \\n          ∑\\n          \\n            j\\n            =\\n            0\\n          \\n          \\n            i\\n          \\n        \\n        \\n          \\n            \\n              (\\n            \\n            \\n              i\\n              j\\n            \\n            \\n              )\\n            \\n          \\n        \\n        (\\n        −\\n        1\\n        \\n          )\\n          \\n            i\\n            −\\n            j\\n          \\n        \\n        \\n          E\\n        \\n        [\\n        (\\n        \\n          ∑\\n          \\n            k\\n            =\\n            1\\n          \\n          \\n            j\\n          \\n        \\n        \\n          Y\\n          \\n            k\\n          \\n        \\n        \\n          )\\n          \\n            n\\n          \\n        \\n        ]\\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {E} \\\\left[W^{n}\\\\right]=\\\\sum _{i=0}^{n}\\\\mathrm {E} \\\\left[{M \\\\choose i}\\\\right]\\\\sum _{j=0}^{i}{i \\\\choose j}(-1)^{i-j}\\\\mathrm {E} [(\\\\sum _{k=1}^{j}Y_{k})^{n}],}\\n  \\nwhere \\n  \\n    \\n      \\n        \\n          E\\n        \\n        [\\n        (\\n        \\n          ∑\\n          \\n            k\\n            =\\n            1\\n          \\n          \\n            j\\n          \\n        \\n        \\n          Y\\n          \\n            k\\n          \\n        \\n        \\n          )\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {E} [(\\\\sum _{k=1}^{j}Y_{k})^{n}]}\\n   is defined as zero for \\n  \\n    \\n      \\n        j\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle j=0}\\n  .\\n\\n\\n=== Symmetric distributions ===\\nIn a symmetric distribution (one that is unaffected by being reflected about its mean), all odd central moments equal zero, because in the formula for the nth moment, each term involving a value of X less than the mean by a certain amount exactly cancels out the term involving a value of X greater than the mean by the same amount.\\n\\n\\n== Multivariate moments ==\\nFor a continuous bivariate probability distribution with probability density function f(x,y) the (j,k) moment about the mean μ = (μX, μY) is\\n\\n  \\n    \\n      \\n        \\n          μ\\n          \\n            j\\n            ,\\n            k\\n          \\n        \\n        =\\n        E\\n        \\u2061\\n        \\n          [\\n          \\n            (\\n            X\\n            −\\n            E\\n            \\u2061\\n            [\\n            X\\n            ]\\n            \\n              )\\n              \\n                j\\n              \\n            \\n            (\\n            Y\\n            −\\n            E\\n            \\u2061\\n            [\\n            Y\\n            ]\\n            \\n              )\\n              \\n                k\\n              \\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∫\\n          \\n            −\\n            ∞\\n          \\n          \\n            +\\n            ∞\\n          \\n        \\n        \\n          ∫\\n          \\n            −\\n            ∞\\n          \\n          \\n            +\\n            ∞\\n          \\n        \\n        (\\n        x\\n        −\\n        \\n          μ\\n          \\n            X\\n          \\n        \\n        \\n          )\\n          \\n            j\\n          \\n        \\n        (\\n        y\\n        −\\n        \\n          μ\\n          \\n            Y\\n          \\n        \\n        \\n          )\\n          \\n            k\\n          \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        \\n        d\\n        x\\n        \\n        d\\n        y\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\mu _{j,k}=\\\\operatorname {E} \\\\left[(X-\\\\operatorname {E} [X])^{j}(Y-\\\\operatorname {E} [Y])^{k}\\\\right]=\\\\int _{-\\\\infty }^{+\\\\infty }\\\\int _{-\\\\infty }^{+\\\\infty }(x-\\\\mu _{X})^{j}(y-\\\\mu _{Y})^{k}f(x,y)\\\\,dx\\\\,dy.}\\n  \\n\\n\\n== See also ==\\nStandardized moment\\nImage moment\\nNormal distribution#Moments\\n\\n\\n== References =='"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipage = wikipedia.WikipediaPage(u'Central moment')\n",
    "text = wikipage.content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable\\'s mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterised. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location. Sets of central moments can be defined for both univariate and multivariate distributions. The nth moment about the mean (or nth central moment) of a real-valued random variable X is the quantity μn := E[(X − E[X])n], where E is the expectation operator. For a continuous univariate probability distribution with probability density function f(x), the nth moment about the mean μ is μ n = E \\u2061 [ ( X − E \\u2061 [ X ] ) n ] = ∫ − ∞ + ∞ ( x − μ ) n f ( x ) d x . For random variables that have no mean, such as the Cauchy distribution, central moments are not defined. The first few central moments have intuitive interpretations: The \"zeroth\" central moment μ0 is 1. The first central moment μ1 is 0 (not to be confused with the first (raw) moment itself, the expected value or mean). The second central moment μ2 is called the variance, and is usually denoted σ2, where σ represents the standard deviation. The third and fourth central moments are used to define the standardized moments which are used to define skewness and kurtosis, respectively. = The nth central moment is translation-invariant, i.e. for any random variable X and any constant c, we have μ n ( X + c ) = μ n ( X ) . For all n, the nth central moment is homogeneous of degree n: μ n ( c X ) = c n μ n ( X ) . Only for n such that n equals 1, 2, or 3 do we have an additivity property for random variables X and Y that are independent: μ n ( X + Y ) = μ n ( X ) + μ n ( Y ) provided n ∈ {1, 2, 3}. A related functional that shares the translation-invariance and homogeneity properties with the nth central moment, but continues to have this additivity property even when n ≥ 4 is the nth cumulant κn(X). For n = 1, the nth cumulant is just the expected value; for n = either 2 or 3, the nth cumulant is just the nth central moment; for n ≥ 4, the nth cumulant is an nth-degree monic polynomial in the first n moments (about zero), and is also a (simpler) nth-degree polynomial in the first n central moments. = Sometimes it is convenient to convert moments about the origin to moments about the mean. The general equation for converting the nth-order moment about the origin to the moment about the mean is μ n = E [ ( X − E [ X ] ) n ] = ∑ j = 0 n ( n j ) ( − 1 ) n − j μ j ′ μ n − j , where μ is the mean of the distribution, and the moment about the origin is given by μ m ′ = ∫ − ∞ + ∞ x m f ( x ) d x = E [ X m ] = ∑ j = 0 m ( m j ) μ j μ m − j . For the cases n = 2, 3, 4 — which are of most interest because of the relations to variance, skewness, and kurtosis, respectively — this formula becomes (noting that μ = μ 1 ′ which is commonly referred to as V a r ( X ) = E [ X 2 ] − ( E [ X ] ) 2 μ 4 = μ 4 ′ − 4 μ μ 3 ′ + 6 μ 2 μ 2 ′ − 3 μ 4 . ... and so on, following Pascal\\'s triangle, i.e. μ 5 = μ 5 ′ − 5 μ μ 4 ′ + 10 μ 2 μ 3 ′ − 10 μ 3 μ 2 ′ + 4 μ 5 . because 5 μ 4 μ 1 ′ − μ 5 μ 0 ′ = 5 μ 4 μ − μ 5 = 5 μ 5 − μ 5 = 4 μ 5 where E [ ( ∑ k = 1 j Y k ) n ] Standardized moment Image moment Normal distribution#Moments '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = strip_markup(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 264.26086950302124,
   "position": {
    "height": "286px",
    "left": "103.08424377441406px",
    "right": "238.790771484375px",
    "top": "139.98641967773438px",
    "width": "579px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
